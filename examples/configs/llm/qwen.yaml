name: qwen
model_name_or_path: Qwen/Qwen2.5-3B

config_class: AutoConfig
config_kwargs:
  trust_remote_code: true

tokenizer_class: AutoTokenizer
tokenizer_kwargs:
  use_fast: true
  trust_remote_code: true

# We'll let the setup_llm function handle the pad_token automatically
tokenizer_special_tokens: {}

model_class: AutoModelForCausalLM
model_kwargs:
  # Use float32 instead of auto to avoid precision issues
  torch_dtype: float32
  trust_remote_code: true
  # Use flash attention if available, but make it optional
  # as some systems might not support it
  attn_implementation: "flash_attention_2"

# Global HF generation configurations
global_gen_kwargs: {}

add_space: false
is_sc: false

use_peft: false
peft:
  # Target modules for Qwen2.5-3B
  # These are the attention and MLP modules in the model
  target_modules: ["q_proj", "v_proj", "o_proj"]

use_quant: false

defaults:
  - quantization: none
  - peft: lora
  - _self_ 